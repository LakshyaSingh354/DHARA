{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lakshya/miniforge3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow_text as text\n",
    "import tensorflow as tf\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_dict = json.load(open('fine-tune-data.json', 'r'))\n",
    "# case_csv = pd.read_csv('data.csv')\n",
    "keys = list(json_file_dict.keys())\n",
    "random.shuffle(keys)\n",
    "\n",
    "split_index = int(len(keys) * 0.8)\n",
    "train_keys = keys[:split_index]\n",
    "test_keys = keys[split_index:]\n",
    "\n",
    "train_dict = {k: json_file_dict[k] for k in train_keys}\n",
    "test_dict = {k: json_file_dict[k] for k in test_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 3093/3093 processed...\r"
     ]
    }
   ],
   "source": [
    "train_df = pd.DataFrame(columns=[\n",
    "    'Query',\n",
    "    'Case',\n",
    "    'Labels'\n",
    "])\n",
    "for i, key in enumerate(train_dict.keys()):\n",
    "    query = key\n",
    "    print(f\"Query {i+1}/{len(train_dict.keys())} processed...\", end='\\r')\n",
    "    for j, case in enumerate(train_dict[key]):\n",
    "        case_text = case[0]\n",
    "        label = case[1]\n",
    "        train_df.loc[len(train_df)] = [query, case, label]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 17:28:42.797161: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-08-29 17:28:42.797190: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-08-29 17:28:42.797200: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-08-29 17:28:42.797217: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-08-29 17:28:42.797228: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "\n",
    "path = kagglehub.model_download(\"tensorflow/bert/tensorFlow2/bert-en-uncased-l-12-h-768-a-12\")\n",
    "bert_model = tf.saved_model.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(query, case, seq_length=512):\n",
    "\n",
    "    combined_text = query + \"[SEP]\" + case\n",
    "\n",
    "    # Load the BERT tokenizer\n",
    "    path = kagglehub.model_download(\"tensorflow/bert/tensorFlow2/en-uncased-preprocess\")\n",
    "    tokenizer = BertTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
    "    \n",
    "    # Tokenize the text\n",
    "    combined_tokens = tokenizer(combined_text, max_length=seq_length, truncation=True, padding='max_length')\n",
    "    \n",
    "    return combined_tokens\n",
    "\n",
    "class ClassifierModel(tf.keras.Model):\n",
    "    def __init__(self, bert_model, seq_length=512):\n",
    "        super(ClassifierModel, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.bert = bert_model\n",
    "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "        self.classifier = tf.keras.layers.Dense(1, activation=None)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "\n",
    "        # input_ids = inputs['input_ids']\n",
    "        # attention_mask = inputs['attention_mask']\n",
    "\n",
    "        # Run the BERT model\n",
    "        outputs = self.bert(inputs)\n",
    "\n",
    "        # Use pooled_output for classification\n",
    "        net = outputs['pooled_output']  # Change 'pooled_output' to 'pooler_output' for Hugging Face models\n",
    "        net = self.dropout(net)\n",
    "        net = self.classifier(net)\n",
    "        \n",
    "        return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfds(dataframe, batch_size=8):\n",
    "    input_word_ids = []\n",
    "    input_type_ids = []\n",
    "    input_mask = []\n",
    "    labels = []\n",
    "\n",
    "    for i in tqdm(range(len(dataframe))):\n",
    "        query = str(dataframe['Query'][i])\n",
    "        case = str(dataframe['Case'][i])\n",
    "        label = dataframe['Labels'][i]\n",
    "        encoder_inputs = preprocess_text(query, case)\n",
    "        \n",
    "        input_word_ids.append(encoder_inputs[\"input_ids\"])\n",
    "        input_type_ids.append(encoder_inputs[\"token_type_ids\"])\n",
    "        input_mask.append(encoder_inputs[\"attention_mask\"])\n",
    "        labels.append(label)\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_word_ids = tf.convert_to_tensor(input_word_ids)\n",
    "    input_type_ids = tf.convert_to_tensor(input_type_ids)\n",
    "    input_mask = tf.convert_to_tensor(input_mask)\n",
    "    labels = tf.convert_to_tensor(labels, dtype=tf.int32)\n",
    "    \n",
    "    # Create TensorFlow Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(({\n",
    "        \"input_word_ids\": input_word_ids,\n",
    "        \"input_type_ids\": input_type_ids,\n",
    "        \"input_mask\": input_mask,\n",
    "    }, labels))\n",
    "    \n",
    "    dataset = dataset.shuffle(1000).batch(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/Users/lakshya/miniforge3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "100%|██████████| 1000/1000 [15:16<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds = create_tfds(train_df.head(1000), batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/xz/bf_n0ls179dgvvt_s7w8nn900000gn/T/ipykernel_81829/2767505419.py:1: save (from tensorflow.python.data.experimental.ops.io) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.save(...)` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/xz/bf_n0ls179dgvvt_s7w8nn900000gn/T/ipykernel_81829/2767505419.py:1: save (from tensorflow.python.data.experimental.ops.io) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.save(...)` instead.\n"
     ]
    }
   ],
   "source": [
    "tf.data.experimental.save(train_ds, 'train_ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassifierModel(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_ds.take(1):\n",
    "    batch_inputs, batch_labels = batch\n",
    "\n",
    "    # Run the model on this batch\n",
    "    initial_outputs = model.call(batch_inputs)\n",
    "\n",
    "    # Print the outputs\n",
    "    print(\"Model outputs before training:\")\n",
    "    print(initial_outputs.numpy())\n",
    "\n",
    "    # Print the corresponding labels for comparison\n",
    "    print(\"\\nCorresponding labels:\")\n",
    "    print(batch_labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.01),\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "            metrics=[tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(query, case, max_length=512):\n",
    "    return tokenizer(query, case, padding='max_length', truncation=True, max_length=max_length)\n",
    "\n",
    "def encode_data(df, max_length=512):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "\n",
    "    for i in tqdm(range((len(df))), desc=\"Encoding data...\"):\n",
    "        encoding = tokenize_function(df['Query'][i], df['Case'][i], max_length=max_length)\n",
    "        input_ids.append(encoding['input_ids'])\n",
    "        attention_masks.append(encoding['attention_mask'])\n",
    "        labels.append(df['Labels'][i])\n",
    "\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "small_df = train_df.head(10)\n",
    "input_ids, attention_masks, labels = encode_data(small_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(input_ids, attention_masks, labels, batch_size=8):\n",
    "    # Convert lists to tensors\n",
    "    input_ids = tf.convert_to_tensor(input_ids)\n",
    "    attention_masks = tf.convert_to_tensor(attention_masks)\n",
    "    labels = tf.convert_to_tensor(labels)\n",
    "\n",
    "    # Create a TensorFlow Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(({\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_masks,\n",
    "    }, labels))\n",
    "\n",
    "    # Shuffle and batch the dataset\n",
    "    dataset = dataset.shuffle(len(labels)).batch(batch_size)\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "train_dataset = create_tf_dataset(input_ids, attention_masks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained('nlpaueb/legal-bert-base-uncased', num_labels=2)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=3e-5),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataset.take(1):\n",
    "    inputs, labels = batch\n",
    "    logits = model(inputs, training=False).logits  # Perform a forward pass through the model\n",
    "\n",
    "# Print the logits\n",
    "print(\"Initialized logits before training:\")\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset, \n",
    "    epochs=10, \n",
    "    verbose=1, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
